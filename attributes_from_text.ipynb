{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attributes-from-text.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xazOq9UZUaoD",
        "YUhJTcVNUrGV",
        "_UlkQnJOWrDO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pati-dev/text-multi-label/blob/master/attributes_from_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2vhYz2qqNzl",
        "colab_type": "text"
      },
      "source": [
        "# Boot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XouXIoPAorWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c0d9dd9-fd8f-42bd-b747-95065c5f8b2c"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorflow_version 2.x\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdgopM6BpPiV",
        "colab_type": "code",
        "outputId": "92d9c72e-d9a9-442c-f36e-21532dff06ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K1lPtY66BLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finalDF = pd.read_csv('/content/drive/My Drive/Search Project/finalDF.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXU6Oz7dDgN",
        "colab_type": "text"
      },
      "source": [
        "Create text to sequence matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr9jWfbRaZQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 1000\n",
        "seq_len = 30\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(finalDF.text)\n",
        "sequences = tokenizer.texts_to_sequences(finalDF.text)\n",
        "x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbc0_PZgb037",
        "colab_type": "code",
        "outputId": "e5699b6e-789a-4408-8366-4e224fddbd1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y = np.array(finalDF.iloc[:,2:])\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1880, 30)\n",
            "(1880, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QZ8PQjoUkc_",
        "colab_type": "text"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C24l44lqeOT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y.astype('float32')\n",
        "x_train, x_tv, y_train, y_tv = train_test_split(x, y, test_size=0.3, random_state=6969)\n",
        "x_valdn, x_test, y_valdn, y_test = train_test_split(x_tv, y_tv, test_size=0.3, random_state=6969)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGcBOTrrXX_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pathToLogs = '/content/drive/My Drive/Search Project/logs/'\n",
        "pathToLogs = '/content/logs/'\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf {pathToLogs}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVxr6IDAZWpY",
        "colab_type": "text"
      },
      "source": [
        "Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Qs76k2qqAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(input_dim=num_words, \n",
        "                                    output_dim=100, \n",
        "                                    input_length=seq_len,\n",
        "                                    dtype='float32'))\n",
        "# model.add(tf.keras.layers.Dropout(0.1))\n",
        "\n",
        "model.add(tf.keras.layers.LSTM(units=32))\n",
        "model.add(tf.keras.layers.LayerNormalization())\n",
        "# model.add(tf.keras.layers.Dropout(0.1))\n",
        "\n",
        "# model.add(tf.keras.layers.Conv1D(filters=32, \n",
        "#                                  kernel_size=2,\n",
        "#                                  strides=1,\n",
        "#                                  activation='relu'))\n",
        "# model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# model.add(tf.keras.layers.Dense(units=1024, \n",
        "#                                activation='relu'))\n",
        "# model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Dropout(0.1))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=32, \n",
        "                               activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "# model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=y_train.shape[1], \n",
        "                               activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFVGSldcfvV4",
        "colab_type": "code",
        "outputId": "f04c73d0-e9b0-44fd-bf64-1ceaced0dc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "valdn_log_dir = 'logs/gradient_tape/' + current_time + '/valdn'\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "valdn_writer = tf.summary.create_file_writer(valdn_log_dir)\n",
        "\n",
        "\n",
        "# Sigmoid Cross-entropy with Logits\n",
        "def computeLoss(labels, logits):\n",
        "  return tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "\n",
        "# Exact Match Ratio or Subset Accuracy\n",
        "def computeAccuracy(truth, predict):\n",
        "  binaryPred = np.where(predict > 0.5, 1., 0.)\n",
        "  acc = accuracy_score(y_true=truth, y_pred=binaryPred, normalize=True)\n",
        "  return acc\n",
        "\n",
        "\n",
        "def train_step(features, labels, optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)):\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    # Compute loss\n",
        "    logits = model(features, training=True)\n",
        "    loss = computeLoss(labels, logits)\n",
        "    loss_value = tf.reduce_mean(loss)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    predictions = model(x_train, training=True)\n",
        "    acc = computeAccuracy(y_train, predictions)\n",
        "\n",
        "  # Compute gradients and minimize using the optimizer\n",
        "  grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  \n",
        "  return loss_value, acc\n",
        "\n",
        "\n",
        "def test_step(model, features, labels):\n",
        "  # Compute loss\n",
        "  predictions = model(features)\n",
        "  loss = computeLoss(labels, predictions)\n",
        "  loss_value = tf.reduce_mean(loss)\n",
        "  # Compute accuracy\n",
        "  acc = computeAccuracy(labels, predictions)\n",
        "\n",
        "  return loss_value, acc\n",
        "\n",
        "\n",
        "def train(epochs, batchSize):\n",
        "  \n",
        "  # Epoch loop\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # Initialize vars\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    numBatches = math.ceil(x_train.shape[0] / batchSize)\n",
        "    \n",
        "    # Batch loop\n",
        "    for batch in range(numBatches):\n",
        "      \n",
        "      # Compute indices\n",
        "      start = batch*batchSize\n",
        "      end = (batch+1)*batchSize\n",
        "      \n",
        "      # Extract batch\n",
        "      features, labels = x_train[start:end], y_train[start:end]\n",
        "      \n",
        "      # Train\n",
        "      batch_loss, batch_acc = train_step(features, labels)\n",
        "      \n",
        "      # Update local loss and accuracy\n",
        "      epoch_loss += batch_loss\n",
        "      epoch_acc += batch_acc\n",
        "    \n",
        "    # Update global loss and accuracy\n",
        "    train_loss = epoch_loss / numBatches\n",
        "    train_acc = epoch_acc / numBatches\n",
        "\n",
        "    # Log training loss and accuracy\n",
        "    with train_writer.as_default():\n",
        "      tf.summary.scalar('training loss', train_loss, step=epoch)\n",
        "      tf.summary.scalar('accuracy', train_acc, step=epoch)\n",
        "\n",
        "    # Validate the model\n",
        "    valdn_loss, valdn_acc = test_step(model, x_valdn, y_valdn)\n",
        "\n",
        "    # Log validation loss and accuracy\n",
        "    with valdn_writer.as_default():\n",
        "      tf.summary.scalar('training loss', valdn_loss, step=epoch)\n",
        "      tf.summary.scalar('accuracy', valdn_acc, step=epoch)\n",
        "    \n",
        "    # Pretty print output\n",
        "    template = 'Epoch {}, Training Loss: {}, Training Accuracy: {}, Validation Loss: {}, Validation Accuracy: {}'\n",
        "    print(template.format(epoch+1,\n",
        "                          train_loss,\n",
        "                          train_acc*100,\n",
        "                          valdn_loss,\n",
        "                          valdn_acc*100))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 100)           100000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32)                17024     \n",
            "_________________________________________________________________\n",
            "layer_normalization (LayerNo (None, 32)                64        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 264       \n",
            "=================================================================\n",
            "Total params: 118,536\n",
            "Trainable params: 118,472\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WAnNI9rfhBy",
        "colab_type": "code",
        "outputId": "da69a383-b552-4ccd-def2-7eb7e01f1813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "train(20, 50)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 0.6565086245536804, Training Accuracy: 1.5704154002026338, Validation Loss: 0.6312613487243652, Validation Accuracy: 2.7918781725888326\n",
            "Epoch 2, Training Loss: 0.6213921904563904, Training Accuracy: 5.507711358775188, Validation Loss: 0.6256600618362427, Validation Accuracy: 2.7918781725888326\n",
            "Epoch 3, Training Loss: 0.6003577709197998, Training Accuracy: 10.29494540132838, Validation Loss: 0.6255175471305847, Validation Accuracy: 4.060913705583756\n",
            "Epoch 4, Training Loss: 0.5826834440231323, Training Accuracy: 15.827986040751995, Validation Loss: 0.6307210326194763, Validation Accuracy: 2.5380710659898478\n",
            "Epoch 5, Training Loss: 0.5683370232582092, Training Accuracy: 22.790723854553647, Validation Loss: 0.6293677091598511, Validation Accuracy: 2.030456852791878\n",
            "Epoch 6, Training Loss: 0.5573309659957886, Training Accuracy: 28.97388269728695, Validation Loss: 0.6151149868965149, Validation Accuracy: 4.822335025380711\n",
            "Epoch 7, Training Loss: 0.5467313528060913, Training Accuracy: 32.93369357199144, Validation Loss: 0.6177705526351929, Validation Accuracy: 2.5380710659898478\n",
            "Epoch 8, Training Loss: 0.5378234386444092, Training Accuracy: 39.24068445345042, Validation Loss: 0.6269954442977905, Validation Accuracy: 1.2690355329949239\n",
            "Epoch 9, Training Loss: 0.527033269405365, Training Accuracy: 46.304739389845764, Validation Loss: 0.6113168001174927, Validation Accuracy: 2.5380710659898478\n",
            "Epoch 10, Training Loss: 0.5165809988975525, Training Accuracy: 52.333108184172026, Validation Loss: 0.6111301183700562, Validation Accuracy: 3.5532994923857872\n",
            "Epoch 11, Training Loss: 0.5068838596343994, Training Accuracy: 58.83991894630193, Validation Loss: 0.6087698936462402, Validation Accuracy: 4.060913705583756\n",
            "Epoch 12, Training Loss: 0.499968022108078, Training Accuracy: 64.07463694697736, Validation Loss: 0.6114002466201782, Validation Accuracy: 3.5532994923857872\n",
            "Epoch 13, Training Loss: 0.4948268234729767, Training Accuracy: 67.74175391196668, Validation Loss: 0.6193646788597107, Validation Accuracy: 3.0456852791878175\n",
            "Epoch 14, Training Loss: 0.49074193835258484, Training Accuracy: 70.87695598333895, Validation Loss: 0.6206015944480896, Validation Accuracy: 1.7766497461928936\n",
            "Epoch 15, Training Loss: 0.48848360776901245, Training Accuracy: 71.88449848024315, Validation Loss: 0.6067875623703003, Validation Accuracy: 1.2690355329949239\n",
            "Epoch 16, Training Loss: 0.48457929491996765, Training Accuracy: 75.1379038613081, Validation Loss: 0.6070668697357178, Validation Accuracy: 1.7766497461928936\n",
            "Epoch 17, Training Loss: 0.4811041057109833, Training Accuracy: 77.46256895193068, Validation Loss: 0.6113166809082031, Validation Accuracy: 2.7918781725888326\n",
            "Epoch 18, Training Loss: 0.4780089557170868, Training Accuracy: 80.18124507486212, Validation Loss: 0.6125463843345642, Validation Accuracy: 2.030456852791878\n",
            "Epoch 19, Training Loss: 0.4751964509487152, Training Accuracy: 81.75447483958119, Validation Loss: 0.6079913973808289, Validation Accuracy: 2.5380710659898478\n",
            "Epoch 20, Training Loss: 0.47336065769195557, Training Accuracy: 82.53968253968253, Validation Loss: 0.6111080050468445, Validation Accuracy: 3.2994923857868024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRCTokEYUQk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape\n",
        "notebook.display(port=6006, height=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCW5FBpaAfsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainPred = model.predict(x_train)\n",
        "trainPred[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5Bgl2wjrJ9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = model.evaluate(x_test, y_test)\n",
        "print('test loss, test acc:', metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZZyleDL8lgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Search Project/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/Search Project/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdt-Ps2egdB6",
        "colab_type": "code",
        "outputId": "7d333bcb-97f7-4f31-dd3c-f216e44c13e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(np.array([[0, 1, 1],\n",
        "                         [1, 1, 0],\n",
        "                         [1, 1, 1],\n",
        "                         [1, 1, 1],\n",
        "                         [0, 0, 1]]),\n",
        "               np.array([[0, 1, 1],\n",
        "                         [0, 1, 1],\n",
        "                         [0, 0, 0],\n",
        "                         [1, 1, 0],\n",
        "                         [0, 0, 1]]))*100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    }
  ]
}